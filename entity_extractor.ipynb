{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c679e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "import google.generativeai as genai\n",
    "import networkx as nx\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c32e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class APIKeyManager:\n",
    "    \"\"\"\n",
    "    Qu·∫£n l√Ω nhi·ªÅu Google API keys v√† t·ª± ƒë·ªông chuy·ªÉn ƒë·ªïi khi g·∫∑p l·ªói.\n",
    "    \n",
    "    Quy t·∫Øc:\n",
    "    - M·ªói key ƒë∆∞·ª£c th·ª≠ t·ªëi ƒëa 2 l·∫ßn\n",
    "    - Sau 2 l·∫ßn l·ªói ‚Üí t·ª± ƒë·ªông chuy·ªÉn key ti·∫øp theo\n",
    "    - H·∫øt key ‚Üí b√°o l·ªói\n",
    "    \"\"\"\n",
    "    \n",
    "    MAX_RETRIES_PER_KEY = 2  # S·ªë l·∫ßn th·ª≠ t·ªëi ƒëa cho m·ªói key\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Kh·ªüi t·∫°o v√† load t·∫•t c·∫£ API keys t·ª´ .env\"\"\"\n",
    "        # Load c√°c API keys t·ª´ environment\n",
    "        self.keys = [\n",
    "            (\"GOOGLE_API_KEY\", os.getenv(\"GOOGLE_API_KEY\")),\n",
    "            (\"GOOGLE_API_KEY_2\", os.getenv(\"GOOGLE_API_KEY_2\")),\n",
    "            (\"GOOGLE_API_KEY_3\", os.getenv(\"GOOGLE_API_KEY_3\")),\n",
    "            (\"GOOGLE_API_KEY_4\", os.getenv(\"GOOGLE_API_KEY_4\")),\n",
    "        ]\n",
    "        \n",
    "        # Ch·ªâ gi·ªØ l·∫°i c√°c key h·ª£p l·ªá (kh√¥ng None)\n",
    "        self.keys = [(name, key) for name, key in self.keys if key]\n",
    "        \n",
    "        if not self.keys:\n",
    "            raise ValueError(\"‚ùå Kh√¥ng t√¨m th·∫•y API key! Ki·ªÉm tra file .env\")\n",
    "        \n",
    "        # Kh·ªüi t·∫°o tr·∫°ng th√°i\n",
    "        self.current_index = 0\n",
    "        self.error_counts = {name: 0 for name, _ in self.keys}  # ƒê·∫øm l·ªói m·ªói key\n",
    "        \n",
    "        print(f\"‚úì Ph√°t hi·ªán {len(self.keys)} API keys\")\n",
    "        self._activate_key(0)\n",
    "    \n",
    "    def _activate_key(self, index):\n",
    "        \"\"\"K√≠ch ho·∫°t API key t·∫°i v·ªã tr√≠ index\"\"\"\n",
    "        if index >= len(self.keys):\n",
    "            raise Exception(\"‚ùå ƒê√£ h·∫øt t·∫•t c·∫£ API keys!\")\n",
    "        \n",
    "        self.current_index = index\n",
    "        key_name, key_value = self.keys[index]\n",
    "        \n",
    "        # C·∫•u h√¨nh Google AI v·ªõi key m·ªõi\n",
    "        genai.configure(api_key=key_value)\n",
    "        \n",
    "        print(f\"üîë ƒêang s·ª≠ d·ª•ng: {key_name} (Key {index + 1}/{len(self.keys)})\")\n",
    "    \n",
    "    def get_current_key(self):\n",
    "        \"\"\"L·∫•y API key hi·ªán t·∫°i\"\"\"\n",
    "        return self.keys[self.current_index][1]\n",
    "    \n",
    "    def get_models(self):\n",
    "        \"\"\"\n",
    "        T·∫°o c√°c model AI v·ªõi API key hi·ªán t·∫°i.\n",
    "        \n",
    "        Returns:\n",
    "            tuple: (model, model_more_temp, model_pro)\n",
    "        \"\"\"\n",
    "        current_key = self.get_current_key()\n",
    "        \n",
    "        model = ChatGoogleGenerativeAI(\n",
    "            model=\"gemini-2.0-flash-lite\", \n",
    "            temperature=0.02,\n",
    "            google_api_key=current_key\n",
    "        )\n",
    "        \n",
    "        model_more_temp = ChatGoogleGenerativeAI(\n",
    "            model=\"gemini-2.0-flash-lite\", \n",
    "            temperature=0.1,\n",
    "            google_api_key=current_key\n",
    "        )\n",
    "        \n",
    "        model_pro = ChatGoogleGenerativeAI(\n",
    "            model=\"gemini-2.5-pro-exp-03-25\", \n",
    "            temperature=0.1,\n",
    "            google_api_key=current_key\n",
    "        )\n",
    "        \n",
    "        return model, model_more_temp, model_pro\n",
    "    \n",
    "    def on_error(self):\n",
    "        \"\"\"\n",
    "        X·ª≠ l√Ω khi g·∫∑p l·ªói API.\n",
    "        \n",
    "        Returns:\n",
    "            bool: True n·∫øu c√≥ th·ªÉ ti·∫øp t·ª•c (ƒë√£ chuy·ªÉn key ho·∫∑c c√≤n retry),\n",
    "                  False n·∫øu ƒë√£ h·∫øt key\n",
    "        \"\"\"\n",
    "        key_name, _ = self.keys[self.current_index]\n",
    "        self.error_counts[key_name] += 1\n",
    "        \n",
    "        error_count = self.error_counts[key_name]\n",
    "        print(f\"‚ö† L·ªói l·∫ßn {error_count} v·ªõi {key_name}\")\n",
    "        \n",
    "        # N·∫øu ƒë√£ ƒë·∫°t gi·ªõi h·∫°n retry cho key n√†y\n",
    "        if error_count >= self.MAX_RETRIES_PER_KEY:\n",
    "            print(f\"‚õî {key_name} ƒë√£ l·ªói {error_count}/{self.MAX_RETRIES_PER_KEY} l·∫ßn\")\n",
    "            \n",
    "            # Th·ª≠ chuy·ªÉn sang key ti·∫øp theo\n",
    "            next_index = self.current_index + 1\n",
    "            \n",
    "            if next_index < len(self.keys):\n",
    "                print(f\"üîÑ Chuy·ªÉn sang key ti·∫øp theo...\")\n",
    "                self._activate_key(next_index)\n",
    "                return True  # ƒê√£ chuy·ªÉn key th√†nh c√¥ng\n",
    "            else:\n",
    "                print(\"‚ùå ƒê√É H·∫æT T·∫§T C·∫¢ API KEYS!\")\n",
    "                return False  # Kh√¥ng c√≤n key n√†o\n",
    "        \n",
    "        # V·∫´n c√≤n retry cho key hi·ªán t·∫°i\n",
    "        return True\n",
    "    \n",
    "    def reset_error_count(self):\n",
    "        \"\"\"Reset ƒë·∫øm l·ªói c·ªßa key hi·ªán t·∫°i (sau khi th√†nh c√¥ng)\"\"\"\n",
    "        key_name, _ = self.keys[self.current_index]\n",
    "        self.error_counts[key_name] = 0\n",
    "\n",
    "# Kh·ªüi t·∫°o API Manager\n",
    "api_manager = APIKeyManager()\n",
    "\n",
    "print(\"‚úì ƒê√£ kh·ªüi t·∫°o APIKeyManager v·ªõi 4 API keys\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af71d02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PROMPT TEMPLATES - Tr√≠ch xu·∫•t th·ª±c th·ªÉ v√† m·ªëi quan h·ªá\n",
    "# ============================================================\n",
    "\n",
    "# Template tr√≠ch xu·∫•t th·ª±c th·ªÉ t·ª´ tin t·ª©c\n",
    "entity_extraction_template = PromptTemplate.from_template(\"\"\"B·∫°n ƒëang l√†m vi·ªác d∆∞·ªõi b·ªëi c·∫£nh ph√¢n t√≠ch kinh t·∫ø. \n",
    "B·∫°n ƒë∆∞·ª£c cho m·ªôt ho·∫∑c nhi·ªÅu b√†i b√°o, bao g·ªìm t·ª±a ƒë·ªÅ v√† m√¥ t·∫£ ng·∫Øn g·ªçn v·ªÅ b√†i b√°o ƒë√≥, ngo√†i ra b·∫°n c√≥\n",
    "th√¥ng tin v·ªÅ ng√†y xu·∫•t b·∫£n c·ªßa b√†i b√°o, v√† lo·∫°i ch·ªß ƒë·ªÅ m√† b√†i b√°o ƒëang ƒë·ªÅ c·∫≠p t·ªõi.\n",
    "\n",
    "H·∫°n ch·∫ø t·∫°o m·ªõi m·ªôt th·ª±c th·ªÉ, ch·ªâ t·∫°o li√™n k·∫øt t·ªõi 5 th·ª±c th·ªÉ. Lu√¥n ∆∞u ti√™n li√™n k·∫øt v·ªõi c√°c th·ª±c th·ªÉ ƒë√£ c√≥: {existing_entities}\n",
    "\n",
    "B·∫°n c·∫ßn ph√¢n t√≠ch b√†i b√°o, ƒë∆∞a ra t√™n c·ªßa nh·ªØng th·ª±c th·ªÉ (v√≠ d·ª• nh∆∞ c·ªï phi·∫øu, ng√†nh ngh·ªÅ, c√¥ng ty, qu·ªëc gia, t·ªânh th√†nh...)\n",
    "s·∫Ω b·ªã ·∫£nh h∆∞·ªüng tr·ª±c ti·∫øp b·ªüi th√¥ng tin c·ªßa b√†i b√°o, theo h∆∞·ªõng t√≠ch c·ª±c ho·∫∑c ti√™u c·ª±c.\n",
    "\n",
    "V·ªõi m·ªói th·ª±c th·ªÉ, ·ªü ph·∫ßn T√™n th·ª±c th·ªÉ, h·∫°n ch·∫ø d√πng d·∫•u ch·∫•m, g·∫°ch ngang, d·∫•u v√† &, d·∫•u ch·∫•m ph·∫©y ;. V√† c·∫ßn ghi th√™m qu·ªëc gia, ƒë·ªãa ph∆∞∆°ng c·ª• th·ªÉ v√† ng√†nh ngh·ªÅ c·ªßa n√≥ (n·∫øu c√≥).\n",
    "T√™n ch·ªâ n√≥i t·ªõi m·ªôt th·ª±c th·ªÉ duy nh·∫•t. Ph·∫ßn T√™n kh√¥ng ƒë∆∞·ª£c qu√° ph·ª©c t·∫°p, ƒë∆°n gi·∫£n nh·∫•t c√≥ th·ªÉ.\n",
    "N·∫øu th·ª±c th·ªÉ n√†o thu·ªôc danh m·ª•c c·ªï phi·∫øu sau: {portfolio}, h√£y ghi r√µ t√™n c·ªï phi·∫øu.\n",
    "V√≠ d·ª•: SSI-Ch·ª©ng kho√°n; Ng√†nh c√¥ng nghi·ªáp Vi·ªát Nam; Ng∆∞·ªùi d√πng M·ªπ; Ng√†nh th√©p Ch√¢u √Å; Ng√†nh du l·ªãch H·∫° Long, ...\n",
    "\n",
    "Ghi nh·ªõ, H·∫°n ch·∫ø t·∫°o m·ªõi m·ªôt th·ª±c th·ªÉ, ch·ªâ t·∫°o li√™n k·∫øt t·ªõi 5 th·ª±c th·ªÉ. Lu√¥n c·ªë li√™n k·∫øt v·ªõi c√°c th·ª±c th·ªÉ ƒë√£ c√≥.\n",
    "\n",
    "Ph·∫ßn gi·∫£i th√≠ch m·ªói th·ª±c th·ªÉ, b·∫Øt bu·ªôc ƒë√°nh gi√° s·ªë li·ªáu ƒë∆∞·ª£c ghi, nhi·ªÅu ho·∫∑c √≠t, tƒÉng ho·∫∑c gi·∫£m, g·∫•p bao nhi√™u l·∫ßn, ...\n",
    "C·∫ßn c·ªë g·∫Øng li√™n k·∫øt v·ªõi nhi·ªÅu th·ª±c th·ªÉ kh√°c. Tuy nhi√™n kh√¥ng suy ngo√†i ph·∫°m vi b√†i b√°o. Kh√¥ng t·ª± ch√®n s·ªë li·ªáu ngo√†i b√†i b√°o.\n",
    "Kh√¥ng d√πng d·∫•u hai ch·∫•m trong ph·∫ßn gi·∫£i th√≠ch, ch·ªâ d√πng hai ch·∫•m : ƒë·ªÉ t√°ch gi·ªØa T√™n th·ª±c th·ªÉ v√† ph·∫ßn gi·∫£i th√≠ch.\n",
    "                                                          \n",
    "ƒê∆∞a ra theo ƒë·ªãnh d·∫°ng sau:\n",
    "[[POSITIVE]]\n",
    "[Entity 1]: [Explanation]\n",
    "...\n",
    "[Entity N]: [Explanation]\n",
    "\n",
    "[[NEGATIVE]]\n",
    "[Entity A]: [Explanation]\n",
    "..\n",
    "[Entity Z]: [Explanation]\n",
    "                                                          \n",
    "M·ªôt v√≠ d·ª• cho b√†i b√°o:\n",
    "\n",
    "(B·∫ÆT ƒê·∫¶U V√ç D·ª§)\n",
    "\n",
    "Ng√†y ƒëƒÉng: 2025-04-07T22:51:00+07:00\n",
    "Lo·∫°i ch·ªß ƒë·ªÅ: Kinh t·∫ø\n",
    "T·ª±a ƒë·ªÅ: N·ªó l·ª±c hi·ªán th·ª±c h√≥a m·ª•c ti√™u th√¥ng tuy·∫øn cao t·ªëc t·ª´ Cao B·∫±ng ƒë·∫øn C√† Mau \n",
    "\n",
    "M√¥ t·∫£: Nh·∫±m ho√†n th√†nh m·ª•c ti√™u ƒë·∫øn nƒÉm 2025 c·∫£ n∆∞·ªõc c√≥ tr√™n 3.000 km ƒë∆∞·ªùng cao t·ªëc, B·ªô X√¢y d·ª±ng, c√°c ƒë·ªãa ph∆∞∆°ng v√† doanh nghi·ªáp ƒëang tri·ªÉn khai thi c√¥ng 28 d·ª± √°n/d·ª± √°n th√†nh ph·∫ßn v·ªõi t·ªïng chi·ªÅu d√†i kho·∫£ng 1.188 km. \n",
    "ƒê·∫øn nay, ti·∫øn ƒë·ªô ƒëa s·ªë c√°c d·ª± √°n b√°m s√°t k·∫ø ho·∫°ch, nhi·ªÅu d·ª± √°n ƒëƒÉng k√Ω ho√†n th√†nh th√¥ng tuy·∫øn trong nƒÉm 2025. C√≥ th·ªÉ n√≥i ng√†nh giao th√¥ng v·∫≠n t·∫£i ƒëang c·ªë g·∫Øng h·∫øt s·ª©c.\n",
    "\n",
    "Danh s√°ch th·ª±c th·ªÉ s·∫Ω b·ªã ·∫£nh h∆∞·ªüng:\n",
    "\n",
    "[[POSITIVE]]\n",
    "B·ªô X√¢y d·ª±ng Vi·ªát Nam: √Åp l·ª±c qu·∫£n l√Ω 28 d·ª± √°n v·ªõi t·ªïng chi·ªÅu d√†i 1188 km, nh·∫±m hi·ªán th·ª±c h√≥a m·ª•c ti√™u ƒë·∫°t 3000 km cao t·ªëc v√†o nƒÉm 2025. S·ªë l∆∞·ª£ng d·ª± √°n tƒÉng g·∫•p nhi·ªÅu l·∫ßn so v·ªõi giai ƒëo·∫°n tr∆∞·ªõc, ƒë√≤i h·ªèi ƒëi·ªÅu ph·ªëi ngu·ªìn l·ª±c v√† ki·ªÉm so√°t ti·∫øn ƒë·ªô ch·∫∑t ch·∫Ω h∆°n.\n",
    "Ch√≠nh quy·ªÅn ƒë·ªãa ph∆∞∆°ng Vi·ªát Nam: Tr·ª±c ti·∫øp ph·ªëi h·ª£p tri·ªÉn khai c√°c d·ª± √°n t·∫°i t·ª´ng t·ªânh th√†nh. C·∫ßn n√¢ng cao nƒÉng l·ª±c qu·∫£n l√Ω v√† s·ª≠ d·ª•ng ng√¢n s√°ch c√¥ng hi·ªáu qu·∫£ ƒë·ªÉ ƒë·∫£m b·∫£o ti·∫øn ƒë·ªô thi c√¥ng theo k·∫ø ho·∫°ch chung qu·ªëc gia.\n",
    "Doanh nghi·ªáp x√¢y d·ª±ng Vi·ªát Nam: ƒê∆∞·ª£c h∆∞·ªüng l·ª£i tr·ª±c ti·∫øp khi nh·∫≠n kh·ªëi l∆∞·ª£ng h·ª£p ƒë·ªìng thi c√¥ng l·ªõn. Doanh thu v√† nƒÉng l·ª±c thi c√¥ng c√≥ th·ªÉ tƒÉng nhanh h∆°n so v·ªõi c√°c giai ƒëo·∫°n tr∆∞·ªõc ƒë√¢y, nh·ªù nhu c·∫ßu ƒë·∫ßu t∆∞ h·∫° t·∫ßng tƒÉng m·∫°nh.\n",
    "\n",
    "[[NEGATIVE]]\n",
    "B·ªô X√¢y d·ª±ng Vi·ªát Nam: R·ªßi ro ch·∫≠m ti·∫øn ƒë·ªô v√† ƒë·ªôi v·ªën n·∫øu ƒëi·ªÅu ph·ªëi kh√¥ng hi·ªáu qu·∫£ do s·ªë l∆∞·ª£ng d·ª± √°n tƒÉng g·∫•p nhi·ªÅu l·∫ßn.\n",
    "Ch√≠nh quy·ªÅn ƒë·ªãa ph∆∞∆°ng Vi·ªát Nam: C√≥ th·ªÉ g·∫∑p kh√≥ khƒÉn trong gi·∫£i ph√≥ng m·∫∑t b·∫±ng v√† qu·∫£n l√Ω v·ªën ƒë·∫ßu t∆∞ n·∫øu nƒÉng l·ª±c t·ªï ch·ª©c y·∫øu.\n",
    "\n",
    "(K·∫æT TH√öC V√ç D·ª§)\n",
    "\n",
    "Ng√†y ƒëƒÉng: {date}\n",
    "Lo·∫°i ch·ªß ƒë·ªÅ: {group}\n",
    "T·ª±a ƒë·ªÅ: {title}\n",
    "\n",
    "M√¥ t·∫£: {description}\n",
    "\n",
    "\n",
    "Danh s√°ch th·ª±c th·ªÉ s·∫Ω b·ªã ·∫£nh h∆∞·ªüng:\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úì ƒê√£ ƒë·ªãnh nghƒ©a entity_extraction_template\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bcc5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "PORTFOLIO_STOCKS = [\"FPT\", \"SSI\", \"VCB\", \"VHM\", \"HPG\", \"GAS\", \"MSN\", \"MWG\", \"GVR\", \"VIC\"]\n",
    "PORTFOLIO_SECTOR = [\"C√¥ng ngh·ªá\", \"Ch·ª©ng kho√°n\", \"Ng√¢n h√†ng\", \"B·∫•t ƒë·ªông s·∫£n\", \"V·∫≠t li·ªáu c∆° b·∫£n\", \n",
    "                     \"D·ªãch v·ª• H·∫° t·∫ßng\", \"Ti√™u d√πng c∆° b·∫£n\", \"B√°n l·∫ª\", \"Ch·∫ø bi·∫øn\", \"B·∫•t ƒë·ªçng s·∫£n\"]\n",
    "BASE_DELAY = 30\n",
    "MAX_RETRIES = 3\n",
    "\n",
    "def create_chains(api_manager):\n",
    "    \"\"\"\n",
    "    T·∫°o chains v·ªõi models t·ª´ APIKeyManager\n",
    "    \"\"\"\n",
    "    model, model_more_temp, model_pro = api_manager.get_models()\n",
    "    \n",
    "    # T·∫°o chain tr√≠ch xu·∫•t th·ª±c th·ªÉ\n",
    "    chain_entity = entity_extraction_template | model\n",
    "    \n",
    "    return chain_entity\n",
    "\n",
    "# Kh·ªüi t·∫°o chain\n",
    "chain_entity = create_chains(api_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b81888",
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_chain_with_retry(chain, prompt, api_manager, base_delay=BASE_DELAY):\n",
    "    \"\"\"\n",
    "    G·ªçi chain v·ªõi c∆° ch·∫ø retry t·ª± ƒë·ªông v√† t√≠ch h·ª£p APIKeyManager\n",
    "    \"\"\"\n",
    "    total_attempts = 0\n",
    "    max_total_attempts = len(api_manager.keys) * api_manager.MAX_RETRIES_PER_KEY\n",
    "    \n",
    "    while total_attempts < max_total_attempts:\n",
    "        try:\n",
    "            # Th·ª≠ g·ªçi API\n",
    "            response = chain.invoke(prompt)\n",
    "            \n",
    "            # Th√†nh c√¥ng -> reset error count\n",
    "            api_manager.reset_error_count()\n",
    "            return response\n",
    "            \n",
    "        except Exception as e:\n",
    "            total_attempts += 1\n",
    "            error_msg = str(e)\n",
    "            \n",
    "            # B√°o l·ªói cho API manager\n",
    "            switched = api_manager.on_error()\n",
    "            \n",
    "            if total_attempts >= max_total_attempts:\n",
    "                print(f\"‚ùå ƒê√£ th·ª≠ t·∫•t c·∫£ {len(api_manager.keys)} API keys ({total_attempts} l·∫ßn) nh∆∞ng v·∫´n l·ªói\")\n",
    "                print(f\"   L·ªói cu·ªëi: {error_msg}\")\n",
    "                return None\n",
    "            \n",
    "            # Ch·ªù tr∆∞·ªõc khi retry\n",
    "            if switched:\n",
    "                delay = base_delay\n",
    "                print(f\"‚è≥ ƒê·ª£i {delay}s tr∆∞·ªõc khi th·ª≠ key m·ªõi...\")\n",
    "            else:\n",
    "                retry_num = api_manager.error_counts.get(api_manager.current_index, 0)\n",
    "                delay = base_delay * (1.5 ** (retry_num - 1))\n",
    "                print(f\"‚è≥ ƒê·ª£i {delay:.0f}s tr∆∞·ªõc khi retry ({retry_num}/{api_manager.MAX_RETRIES_PER_KEY})...\")\n",
    "            \n",
    "            time.sleep(delay)\n",
    "\n",
    "def parse_entity_response(response):\n",
    "    \"\"\"\n",
    "    Ph√¢n t√≠ch response t·ª´ entity extraction prompt\n",
    "    \n",
    "    Returns:\n",
    "        dict: {\"POSITIVE\": [(entity, explanation), ...], \"NEGATIVE\": [(entity, explanation), ...]}\n",
    "    \"\"\"\n",
    "    if response is None:\n",
    "        print(\"Response is None\")\n",
    "        return {\"POSITIVE\": [], \"NEGATIVE\": []}\n",
    "        \n",
    "    sections = {\"POSITIVE\": [], \"NEGATIVE\": []}\n",
    "    current_section = None\n",
    "    str_resp = response.content\n",
    "    \n",
    "    for line in str(str_resp).splitlines():\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        if \"[[POSITIVE]]\" in line.upper():\n",
    "            current_section = \"POSITIVE\"\n",
    "            continue\n",
    "        if \"[[NEGATIVE]]\" in line.upper():\n",
    "            current_section = \"NEGATIVE\"\n",
    "            continue\n",
    "        if current_section and ':' in line:\n",
    "            entity = line.split(\":\", 1)[0].strip()\n",
    "            # Skip invalid entities\n",
    "            if not entity or \"kh√¥ng c√≥ th·ª±c th·ªÉ n√†o\" in entity.lower():\n",
    "                continue\n",
    "            # content = all line except entity\n",
    "            content = line.split(entity, 1)[-1].strip(':').strip()\n",
    "            sections[current_section].append((entity, content))\n",
    "\n",
    "    return sections\n",
    "\n",
    "def merge_entity(entity, canonical_set):\n",
    "    \"\"\"\n",
    "    Tr·∫£ v·ªÅ phi√™n b·∫£n canonical c·ªßa entity n·∫øu ƒë√£ t·ªìn t·∫°i (case-insensitive),\n",
    "    n·∫øu kh√¥ng th√¨ th√™m v√† tr·∫£ v·ªÅ entity m·ªõi.\n",
    "    \"\"\"\n",
    "    normalized_entity = str(entity).strip('[').strip(']').strip(' ').lower()\n",
    "    for exist in canonical_set:\n",
    "        if exist.lower() == normalized_entity:\n",
    "            return exist\n",
    "    canonical_set.add(normalized_entity)\n",
    "    return normalized_entity\n",
    "\n",
    "def graph_entities_to_str(G, max_entities=50):\n",
    "    \"\"\"\n",
    "    Chuy·ªÉn ƒë·ªïi c√°c entities trong graph th√†nh chu·ªói ƒë·ªÉ ƒë∆∞a v√†o prompt\n",
    "    \"\"\"\n",
    "    entities = [node for node in G.nodes() if not node.startswith(\"Article_\")]\n",
    "    # Gi·ªõi h·∫°n s·ªë l∆∞·ª£ng ƒë·ªÉ kh√¥ng l√†m prompt qu√° d√†i\n",
    "    entities = entities[:max_entities]\n",
    "    return \", \".join(entities) if entities else \"Ch∆∞a c√≥ th·ª±c th·ªÉ n√†o\"\n",
    "\n",
    "print(\"‚úì ƒê√£ ƒë·ªãnh nghƒ©a c√°c h√†m ti·ªán √≠ch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec082aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# H√ÄM CH√çNH - Tr√≠ch xu·∫•t th·ª±c th·ªÉ t·ª´ tin t·ª©c\n",
    "# ============================================================\n",
    "\n",
    "def extract_entities_from_news(\n",
    "    csv_path=\"summarized_news_with_stocks.csv\",\n",
    "    output_path=\"entities_extracted.csv\",\n",
    "    start_date=None,\n",
    "    end_date=None,\n",
    "    max_articles=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Tr√≠ch xu·∫•t th·ª±c th·ªÉ t·ª´ tin t·ª©c ƒë√£ t√≥m t·∫Øt\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    csv_path : str\n",
    "        ƒê∆∞·ªùng d·∫´n ƒë·∫øn file CSV ch·ª©a tin t·ª©c ƒë√£ t√≥m t·∫Øt\n",
    "    output_path : str\n",
    "        ƒê∆∞·ªùng d·∫´n file CSV k·∫øt qu·∫£\n",
    "    start_date : str, optional\n",
    "        Ng√†y b·∫Øt ƒë·∫ßu (format: YYYY-MM-DD)\n",
    "    end_date : str, optional\n",
    "        Ng√†y k·∫øt th√∫c (format: YYYY-MM-DD)\n",
    "    max_articles : int, optional\n",
    "        S·ªë l∆∞·ª£ng b√†i b√°o t·ªëi ƒëa ƒë·ªÉ x·ª≠ l√Ω (ƒë·ªÉ test)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple: (entities_df, graph, canonical_entities)\n",
    "        - entities_df: DataFrame ch·ª©a c√°c th·ª±c th·ªÉ ƒë√£ tr√≠ch xu·∫•t\n",
    "        - graph: NetworkX graph ch·ª©a m·ªëi quan h·ªá\n",
    "        - canonical_entities: Set c√°c th·ª±c th·ªÉ canonical\n",
    "    \"\"\"\n",
    "    print(f\"üìñ ƒêang ƒë·ªçc d·ªØ li·ªáu t·ª´ {csv_path}...\")\n",
    "    \n",
    "    # ƒê·ªçc d·ªØ li·ªáu tin t·ª©c ƒë√£ t√≥m t·∫Øt\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(f\"‚úì ƒê√£ ƒë·ªçc {len(df)} tin t·ª©c\")\n",
    "    \n",
    "    # Chuy·ªÉn ƒë·ªïi c·ªôt date sang datetime\n",
    "    df['parsed_date'] = pd.to_datetime(df['date'])\n",
    "    df['only_date'] = df['parsed_date'].dt.date\n",
    "    \n",
    "    # L·ªçc theo kho·∫£ng th·ªùi gian\n",
    "    if start_date:\n",
    "        start_dt = pd.to_datetime(start_date).date()\n",
    "        df = df[df['only_date'] >= start_dt]\n",
    "        print(f\"‚úì L·ªçc t·ª´ ng√†y {start_date}: c√≤n {len(df)} tin\")\n",
    "    \n",
    "    if end_date:\n",
    "        end_dt = pd.to_datetime(end_date).date()\n",
    "        df = df[df['only_date'] <= end_dt]\n",
    "        print(f\"‚úì L·ªçc ƒë·∫øn ng√†y {end_date}: c√≤n {len(df)} tin\")\n",
    "    \n",
    "    # Gi·ªõi h·∫°n s·ªë l∆∞·ª£ng n·∫øu c·∫ßn (ƒë·ªÉ test)\n",
    "    if max_articles:\n",
    "        df = df.head(max_articles)\n",
    "        print(f\"‚úì Gi·ªõi h·∫°n xu·ªëng {len(df)} tin ƒë·ªÉ x·ª≠ l√Ω\")\n",
    "    \n",
    "    # S·∫Øp x·∫øp theo th·ªùi gian\n",
    "    df = df.sort_values('date')\n",
    "    \n",
    "    # Kh·ªüi t·∫°o graph v√† canonical entities\n",
    "    G = nx.DiGraph()\n",
    "    canonical_entities = set()\n",
    "    \n",
    "    # Build portfolio string\n",
    "    portfolio_str_full = \", \".join([f\"{stock}-{sector}\" for stock, sector in zip(PORTFOLIO_STOCKS, PORTFOLIO_SECTOR)])\n",
    "    \n",
    "    # K·∫øt qu·∫£\n",
    "    all_entities = []\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üîç B·∫ÆT ƒê·∫¶U TR√çCH XU·∫§T TH·ª∞C TH·ªÇ\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # X·ª≠ l√Ω t·ª´ng b√†i b√°o\n",
    "    for idx, row in df.iterrows():\n",
    "        article_idx = idx + 1\n",
    "        article_node = f\"Article_{article_idx}: {row['title']}\"\n",
    "        article_timestamp = row['parsed_date']\n",
    "        \n",
    "        # Th√™m node b√†i b√°o v√†o graph\n",
    "        if not G.has_node(article_node):\n",
    "            G.add_node(article_node, type=\"article\", timestamp=article_timestamp)\n",
    "        \n",
    "        print(f\"[{article_idx}/{len(df)}] üì∞ {row['title'][:60]}...\")\n",
    "        \n",
    "        # T·∫°o group t·ª´ stockCodes n·∫øu c√≥, kh√¥ng th√¨ ƒë·ªÉ \"Chung\"\n",
    "        group = row.get('stockCodes', 'Chung') if row.get('stockCodes') else 'Chung'\n",
    "        \n",
    "        # Phase 1: Extract initial entities\n",
    "        max_entity_retries = MAX_RETRIES\n",
    "        entity_retry_count = 0\n",
    "        entities_dict = {\"POSITIVE\": [], \"NEGATIVE\": []}\n",
    "        \n",
    "        while entity_retry_count < max_entity_retries:\n",
    "            prompt_text = {\n",
    "                \"portfolio\": portfolio_str_full,\n",
    "                \"date\": row['date'],\n",
    "                \"group\": group,\n",
    "                \"title\": row['title'],\n",
    "                \"description\": row['description'],\n",
    "                \"existing_entities\": graph_entities_to_str(G)\n",
    "            }\n",
    "            \n",
    "            response_text = invoke_chain_with_retry(chain_entity, prompt_text, api_manager)\n",
    "            time.sleep(1)  # Rate limiting\n",
    "            \n",
    "            if response_text is None:\n",
    "                print(f\"   ‚ùå B·ªè qua tin {article_idx} do l·ªói API\")\n",
    "                break\n",
    "            \n",
    "            entities_dict = parse_entity_response(response_text)\n",
    "            \n",
    "            # Check if we got any entities\n",
    "            total_entities = len(entities_dict.get(\"POSITIVE\", [])) + len(entities_dict.get(\"NEGATIVE\", []))\n",
    "            if total_entities > 0:\n",
    "                print(f\"   ‚úì Tr√≠ch xu·∫•t ƒë∆∞·ª£c {total_entities} th·ª±c th·ªÉ\")\n",
    "                break\n",
    "                \n",
    "            entity_retry_count += 1\n",
    "            print(f\"   ‚ö† Kh√¥ng c√≥ th·ª±c th·ªÉ. Th·ª≠ l·∫°i {entity_retry_count}/{max_entity_retries}\")\n",
    "            time.sleep(BASE_DELAY)\n",
    "        \n",
    "        if entity_retry_count == max_entity_retries and total_entities == 0:\n",
    "            print(f\"   ‚ùå Th·∫•t b·∫°i sau {max_entity_retries} l·∫ßn th·ª≠\")\n",
    "            continue\n",
    "        \n",
    "        # Process entities\n",
    "        for impact in [\"POSITIVE\", \"NEGATIVE\"]:\n",
    "            for ent, content in entities_dict.get(impact, []):\n",
    "                # Skip invalid entities\n",
    "                if not ent or \"kh√¥ng c√≥ th·ª±c th·ªÉ n√†o\" in ent.lower():\n",
    "                    continue\n",
    "                \n",
    "                # Normalize entity\n",
    "                canon_ent = merge_entity(ent, canonical_entities)\n",
    "                \n",
    "                # Determine node type\n",
    "                node_type = \"stock\" if any(str(canon_ent).lower().find(stock.lower()) != -1 for stock in PORTFOLIO_STOCKS) else \"entity\"\n",
    "                \n",
    "                # Add node to graph\n",
    "                if not G.has_node(canon_ent):\n",
    "                    G.add_node(canon_ent, type=node_type, timestamp=article_timestamp)\n",
    "                \n",
    "                # Add edge from article to entity\n",
    "                if not G.has_edge(article_node, canon_ent):\n",
    "                    G.add_edge(article_node, canon_ent, impact=impact, timestamp=article_timestamp)\n",
    "                \n",
    "                # L∆∞u v√†o k·∫øt qu·∫£\n",
    "                all_entities.append({\n",
    "                    \"article_id\": article_idx,\n",
    "                    \"article_title\": row['title'],\n",
    "                    \"date\": row['date'],\n",
    "                    \"entity\": canon_ent,\n",
    "                    \"entity_type\": node_type,\n",
    "                    \"impact\": impact,\n",
    "                    \"explanation\": content\n",
    "                })\n",
    "    \n",
    "    # T·∫°o DataFrame k·∫øt qu·∫£\n",
    "    entities_df = pd.DataFrame(all_entities)\n",
    "    \n",
    "    # L∆∞u file\n",
    "    entities_df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"‚úÖ HO√ÄN TH√ÄNH!\")\n",
    "    print(f\"üìä T·ªïng s·ªë entities: {len(entities_df)}\")\n",
    "    print(f\"üîπ Unique entities: {len(canonical_entities)}\")\n",
    "    print(f\"üìà Graph nodes: {len(G.nodes())}\")\n",
    "    print(f\"üîó Graph edges: {len(G.edges())}\")\n",
    "    print(f\"üíæ ƒê√£ l∆∞u v√†o: {output_path}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    return entities_df, G, canonical_entities\n",
    "\n",
    "print(\"‚úì ƒê√£ ƒë·ªãnh nghƒ©a h√†m extract_entities_from_news()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea251fe5",
   "metadata": {},
   "source": [
    "## üöÄ Ch·∫°y tr√≠ch xu·∫•t th·ª±c th·ªÉ\n",
    "\n",
    "### H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng:\n",
    "\n",
    "1. **Test v·ªõi s·ªë l∆∞·ª£ng nh·ªè:** Th·ª≠ v·ªõi `max_articles=10` ƒë·ªÉ ki·ªÉm tra\n",
    "2. **Ch·∫°y ƒë·∫ßy ƒë·ªß:** B·ªè `max_articles` ƒë·ªÉ x·ª≠ l√Ω to√†n b·ªô\n",
    "3. **L·ªçc theo th·ªùi gian:** D√πng `start_date` v√† `end_date`\n",
    "\n",
    "### V√≠ d·ª•:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb6e205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test v·ªõi 10 tin t·ª©c ƒë·∫ßu ti√™n\n",
    "entities_df, G, canonical_entities = extract_entities_from_news(\n",
    "    csv_path=\"summarized_news_with_stocks.csv\",\n",
    "    output_path=\"entities_extracted.csv\",\n",
    "    start_date=\"2022-09-30\",  # Ng√†y b·∫Øt ƒë·∫ßu\n",
    "    end_date=None,             # None = ƒë·∫øn cu·ªëi     \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a79ca8e",
   "metadata": {},
   "source": [
    "## üìä Ph√¢n t√≠ch k·∫øt qu·∫£\n",
    "\n",
    "Xem c√°c th·ª±c th·ªÉ ƒë√£ tr√≠ch xu·∫•t:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6611160e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xem t·ªïng quan\n",
    "print(f\"üìä T·ªîNG QUAN K·∫æT QU·∫¢\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "print(f\"T·ªïng s·ªë entities tr√≠ch xu·∫•t: {len(entities_df)}\")\n",
    "print(f\"Unique entities: {len(canonical_entities)}\")\n",
    "print(f\"Graph nodes: {len(G.nodes())}\")\n",
    "print(f\"Graph edges: {len(G.edges())}\")\n",
    "\n",
    "print(f\"\\nüìà PH√ÇN LO·∫†I THEO IMPACT:\\n\")\n",
    "print(entities_df['impact'].value_counts())\n",
    "\n",
    "print(f\"\\nüìå PH√ÇN LO·∫†I THEO ENTITY TYPE:\\n\")\n",
    "print(entities_df['entity_type'].value_counts())\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"M·∫™U 10 ENTITIES ƒê·∫¶U TI√äN:\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "for idx, row in entities_df.head(10).iterrows():\n",
    "    print(f\"[{row['article_id']}] üì∞ {row['article_title'][:50]}...\")\n",
    "    print(f\"üè∑Ô∏è  Entity: {row['entity']} ({row['entity_type']})\")\n",
    "    print(f\"{'‚úÖ' if row['impact'] == 'POSITIVE' else '‚ùå'} Impact: {row['impact']}\")\n",
    "    print(f\"üìù {row['explanation'][:100]}...\")\n",
    "    print(f\"{'-'*60}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c143bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xem c√°c entity ƒë∆∞·ª£c ƒë·ªÅ c·∫≠p nhi·ªÅu nh·∫•t\n",
    "from collections import Counter\n",
    "\n",
    "entity_counts = Counter(entities_df['entity'])\n",
    "\n",
    "print(f\"üî• TOP 15 ENTITIES ƒê∆Ø·ª¢C ƒê·ªÄ C·∫¨P NHI·ªÄU NH·∫§T:\\n\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "for i, (entity, count) in enumerate(entity_counts.most_common(15), 1):\n",
    "    # ƒê·∫øm positive v√† negative\n",
    "    pos_count = len(entities_df[(entities_df['entity'] == entity) & (entities_df['impact'] == 'POSITIVE')])\n",
    "    neg_count = len(entities_df[(entities_df['entity'] == entity) & (entities_df['impact'] == 'NEGATIVE')])\n",
    "    \n",
    "    print(f\"{i:2d}. {entity}\")\n",
    "    print(f\"    T·ªïng: {count} l·∫ßn | ‚úÖ {pos_count} | ‚ùå {neg_count}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2d87c6",
   "metadata": {},
   "source": [
    "## üîç T√¨m ki·∫øm v√† ph√¢n t√≠ch\n",
    "\n",
    "### T√¨m th√¥ng tin v·ªÅ m·ªôt entity c·ª• th·ªÉ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2a08a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_entity(entity_name, entities_df):\n",
    "    \"\"\"\n",
    "    T√¨m ki·∫øm t·∫•t c·∫£ th√¥ng tin v·ªÅ m·ªôt entity\n",
    "    \"\"\"\n",
    "    # T√¨m ki·∫øm case-insensitive\n",
    "    results = entities_df[entities_df['entity'].str.lower().str.contains(entity_name.lower(), na=False)]\n",
    "    \n",
    "    if len(results) == 0:\n",
    "        print(f\"‚ùå Kh√¥ng t√¨m th·∫•y entity: {entity_name}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"üîç T√åM TH·∫§Y {len(results)} MENTIONS V·ªÄ '{entity_name.upper()}'\\n\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    for idx, row in results.iterrows():\n",
    "        print(f\"[{row['article_id']}] üì∞ {row['article_title']}\")\n",
    "        print(f\"üìÖ {row['date']}\")\n",
    "        print(f\"üè∑Ô∏è  {row['entity']} ({row['entity_type']})\")\n",
    "        print(f\"{'‚úÖ' if row['impact'] == 'POSITIVE' else '‚ùå'} {row['impact']}\")\n",
    "        print(f\"üìù {row['explanation']}\")\n",
    "        print(f\"{'-'*60}\\n\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# V√≠ d·ª•: T√¨m th√¥ng tin v·ªÅ FPT\n",
    "# search_entity(\"FPT\", entities_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505062d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ph√¢n t√≠ch Graph: Xem c√°c entities k·∫øt n·ªëi v·ªõi nhau\n",
    "def analyze_graph(G):\n",
    "    \"\"\"\n",
    "    Ph√¢n t√≠ch knowledge graph ƒë√£ x√¢y d·ª±ng\n",
    "    \"\"\"\n",
    "    print(f\"üìä PH√ÇN T√çCH KNOWLEDGE GRAPH\\n\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Th·ªëng k√™ c∆° b·∫£n\n",
    "    print(f\"T·ªïng s·ªë nodes: {len(G.nodes())}\")\n",
    "    print(f\"T·ªïng s·ªë edges: {len(G.edges())}\")\n",
    "    \n",
    "    # Ph√¢n lo·∫°i nodes\n",
    "    node_types = {}\n",
    "    for node, data in G.nodes(data=True):\n",
    "        node_type = data.get('type', 'unknown')\n",
    "        node_types[node_type] = node_types.get(node_type, 0) + 1\n",
    "    \n",
    "    print(f\"\\nüìå Ph√¢n lo·∫°i nodes:\")\n",
    "    for ntype, count in node_types.items():\n",
    "        print(f\"   {ntype}: {count}\")\n",
    "    \n",
    "    # T√¨m nodes c√≥ nhi·ªÅu k·∫øt n·ªëi nh·∫•t (degree centrality)\n",
    "    entity_nodes = [n for n, d in G.nodes(data=True) if d.get('type') != 'article']\n",
    "    \n",
    "    if entity_nodes:\n",
    "        # In-degree: s·ªë l∆∞·ª£ng b√†i b√°o li√™n k·∫øt ƒë·∫øn entity n√†y\n",
    "        in_degrees = [(node, G.in_degree(node)) for node in entity_nodes]\n",
    "        in_degrees.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        print(f\"\\nüî• TOP 10 ENTITIES ƒê∆Ø·ª¢C NH·∫ÆC ƒê·∫æN NHI·ªÄU NH·∫§T (b·ªüi c√°c b√†i b√°o):\\n\")\n",
    "        for i, (node, degree) in enumerate(in_degrees[:10], 1):\n",
    "            print(f\"{i:2d}. {node}: {degree} b√†i b√°o\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "\n",
    "# Ch·∫°y ph√¢n t√≠ch\n",
    "analyze_graph(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b70144",
   "metadata": {},
   "source": [
    "## üíæ L∆∞u Graph ƒë·ªÉ s·ª≠ d·ª•ng sau\n",
    "\n",
    "L∆∞u graph v√†o file pickle ƒë·ªÉ s·ª≠ d·ª•ng cho c√°c b∆∞·ªõc ti·∫øp theo (relation extraction, attention mechanism...):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcc08b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# L∆∞u graph\n",
    "with open('knowledge_graph.pkl', 'wb') as f:\n",
    "    pickle.dump(G, f)\n",
    "print(\"‚úì ƒê√£ l∆∞u knowledge graph v√†o 'knowledge_graph.pkl'\")\n",
    "\n",
    "# L∆∞u canonical entities\n",
    "with open('canonical_entities.pkl', 'wb') as f:\n",
    "    pickle.dump(canonical_entities, f)\n",
    "print(\"‚úì ƒê√£ l∆∞u canonical_entities v√†o 'canonical_entities.pkl'\")\n",
    "\n",
    "# ƒê·ªÉ load l·∫°i sau:\n",
    "# with open('knowledge_graph.pkl', 'rb') as f:\n",
    "#     G = pickle.load(f)\n",
    "# with open('canonical_entities.pkl', 'rb') as f:\n",
    "#     canonical_entities = pickle.load(f)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
